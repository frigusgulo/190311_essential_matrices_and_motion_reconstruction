{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential matrices\n",
    "In the last notebook you computed point correspondences between images using SIFT descriptors and a brute force matching scheme similar to what was used for image stitching.  With these correspondences in hand, we could, in principle, apply the triangulation code developed earlier in order to find the 3D location of all these points.  Or could we?  Triangulation was possible because we already had pre-defined ground control points with which to compute a camera matrix.  However, producing these ground control points is extremely laborious: for each image that we might care to analyze, we must find (manually) at least 3 (and sometimes more) examples for which we know a correspondence between real world coordinates and image coordinates.  This is often not desirable (or even possible).  Let's look at an example of SIFT corresponces, filtered by the ratio test, of a scene in my office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import piexif\n",
    "\n",
    "I_1 = plt.imread('pens_0.jpg')\n",
    "I_2 = plt.imread('pens_1.jpg')\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "kp1,des1 = sift.detectAndCompute(I_1,None)\n",
    "kp2,des2 = sift.detectAndCompute(I_2,None)\n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.6*n.distance:\n",
    "        good.append(m)\n",
    "    \n",
    "u1 = []\n",
    "u2 = []\n",
    "\n",
    "for m in good:\n",
    "    u1.append(kp1[m.queryIdx].pt)\n",
    "    u2.append(kp2[m.trainIdx].pt)\n",
    "    \n",
    "u1 = np.array(u1)\n",
    "u2 = np.array(u2)\n",
    "\n",
    "#Make homogeneous\n",
    "u1 = np.c_[u1,np.ones(u1.shape[0])]\n",
    "u2 = np.c_[u2,np.ones(u2.shape[0])]\n",
    "\n",
    "\n",
    "skip = 10\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "I_new = np.zeros((h,2*w,3)).astype(int)\n",
    "I_new[:,:w,:] = I_1\n",
    "I_new[:,w:,:] = I_2\n",
    "plt.imshow(I_new)\n",
    "plt.scatter(u1[::skip,0],u1[::skip,1])\n",
    "plt.scatter(u2[::skip,0]+w,u2[::skip,1])\n",
    "[plt.plot([u1[0],u2[0]+w],[u1[1],u2[1]]) for u1,u2 in zip(u1[::skip],u2[::skip])]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obviously some mismatches, but hopefully we can deal with that later by filtering out outliers.  \n",
    "\n",
    "With these correspondences in hand, we can work towards finding a relationship between the relative geometry of the cameras with which these two images were taken.  This is not as simple as finding a homography, because not only did the camera *rotate*, but it translated as well.  As such, points in the first image will not generally map to points in the second image.  Instead, points in the first image will map to *lines* in the second image.  This is easily understood by looking at the following figure:\n",
    "<img src=epipolar.jpg>\n",
    "If $C$ is the optical center of camera one, and $\\mathbf{x}$ is the location of a point of interest on camera one's imaging plane in generalized image coordinates, then these two locations form a ray, a line which shoots out from the camera and intersects all of the places in world coordinates that will map to that location on image one's imaging plane.  What does that ray look like in image two?  It is, of course, a line (unless the camera centers are collocated).  We can write this property mathematically as\n",
    "$$ \n",
    "\\mathbf{E} \\mathbf{x}_1 = \\mathbf{l}_2,\n",
    "$$\n",
    "where $\\mathbf{l}_2$ are the coefficients of the line in the second image, i.e.\n",
    "$$\n",
    "ax + by + c = \\mathbf{l} \\cdot \\mathbf{x} = 0, \n",
    "$$\n",
    "and $\\mathbf{E}$ is called the *essential matrix*.  As it turns out, the essential matrix contains all of the information we need for recovering the relative geometry between two images, and in fact has the property that \n",
    "$$\n",
    "\\mathbf{E} = [\\mathbf{t}]_\\times \\mathbf{R},\n",
    "$$\n",
    "where $\\mathbf{R}$ is the rotation matrix between the two cameras and $[\\mathbf{t}]_\\times$ is the cross product acting on the translation vector, i.e. \n",
    "$$\n",
    "[\\mathbf{t}]_\\times = \\begin{bmatrix} 0 & -t_Z & t_Y \\\\\n",
    "                                      t_X & 0 & -t_X \\\\\n",
    "                                      -t_Y & t_X & 0 \\end{bmatrix}.\n",
    "$$                                     \n",
    "Both $\\mathbf{R}$ and $\\mathbf{t}$ can be recovered from the essential matrix, the former exactly, and the latter up to a scale.                        \n",
    "\n",
    "Note that the essential matrix is defined in terms of generalized image coordinates, rather than normal image coordinates, which is to say that the influence of focal lengths and camera center positions have been removed.  How do we compute these coordinates?  Recall that image coordinates are related to generalized image coordinates by \n",
    "$$\n",
    "\\mathbf{u} = \\mathbf{K} \\mathbf{x},\n",
    "$$\n",
    "where $\\mathbf{K}$ is the so-called camera matrix\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} f & 0 & c_u \\\\\n",
    "                             0 & f & c_v \\\\\n",
    "                             0 & 0 & 1 \\end{bmatrix}.\n",
    "$$\n",
    "$\\mathbf{K}$ is easily invertible, so we have that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,d = I_1.shape\n",
    "f = exif_1['Exif'][piexif.ExifIFD.FocalLengthIn35mmFilm]/36*w\n",
    "cu = w//2\n",
    "cv = h//2\n",
    "\n",
    "K_cam = np.array([[f,0,cu],[0,f,cv],[0,0,1]])\n",
    "K_inv = np.linalg.inv(K_cam)\n",
    "x1 = u1 @ K_inv.T\n",
    "x2 = u2 @ K_inv.T \n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the $\\mathbf{x}$ values are approximately scaled around 1, which will be numerically helpful when computing the essential matrix.  \n",
    "\n",
    "Back to the essential matrix: how do we find it?  Recall that \n",
    "$$ \n",
    "\\mathbf{E} \\mathbf{x}_1 = \\mathbf{l}_2.\n",
    "$$\n",
    "If we knew $\\mathbf{l}_2$, we could back out $\\mathbf{E}$.  Unfortunately, we don't.  However, because we have point correspondences, we know something almost as good: the location in image 2 of a point that falls on $\\mathbf{l_2}$, which is to say that we know a point $\\mathbf{x}_2$, such that \n",
    "$$\n",
    "\\mathbf{l}_2 \\cdot \\mathbf{x}_2 = 0 (=) \\mathbf{x}_2^T \\mathbf{l}_2,\n",
    "$$\n",
    "by the definition of $\\mathbf{l}_2$.  Left multiplying the expression for the essential matrix by $\\mathbf{x}_2^T$, we get\n",
    "$$\n",
    "\\mathbf{x}_2^T \\mathbf{E} \\mathbf{x}_1 = \\mathbf{x}_2^T \\mathbf{l_2} = 0.\n",
    "$$\n",
    "If we multiply out the left side of this thing, the coefficients of $\\mathbf{E}$ appear linearly (See Szeliski, eq. 7.13).  Thus, if have 8 point correspondences (as in the homography, this matrix is only defined up to scale), then we can recover the entries of $\\mathbf{E}$.  In fact, there are even better algorithms which allow us to find $\\mathbf{E}$ using as few as 5 point correspondences.  Note that as in the case of computing homographies, this process is sensitive to outliers: thus it is beneficial to use RANSAC or something similar to find a model that maximizes the number of inliers while discarding points that do not fit the model.  \n",
    "\n",
    "This would be alot to code ourselves: Fortunately, OpenCV has an excellent method that will, given point correspondences, perform a 5-point algorithm for finding $\\mathbf{E}$ wrapped in RANSAC for us.  Because the process is so similar to computing homographies, we will use this instead of coding it ourselves.  It can be called as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E,inliers = cv2.findEssentialMat(x1[:,:2],x2[:,:2],np.eye(3),method=cv2.RANSAC,threshold=1e-3)\n",
    "inliers = inliers.ravel().astype(bool)\n",
    "print(E,inliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function call, the first two arguments are our corresponding points in generalized, non-homogeneous, camera coordinates (hence we drop the last column of ones).  The third argument is a camera matrix: in principle, we could give this function $\\mathbf{u}_1, \\mathbf{u}_2$ along with the camera matrix instead of $\\mathbf{x}_1,\\mathbf{x}_2$, but my experimentation has shown that this leads to poor results because of the ill-conditioning of the resulting linear system of equations.  Since we are providing coordinates which have already had the camera intrinsics removed, we give it the identity matrix.  The fourth argument specifies that we want to use RANSAC for outlier detection, and the threshold argument is the RANSAC outlier detection threshold: because we're in generalized coordinates, this should be on the order of 1-3 divide by the number of pixels.\n",
    "\n",
    "The algorithm returns the computed essential matrix, as well as a mask of points which successfully passed the outlier test.  We can plot the resulting points (in camera coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 10\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "I_new = np.zeros((h,2*w,3)).astype(int)\n",
    "I_new[:,:w,:] = I_1\n",
    "I_new[:,w:,:] = I_2\n",
    "plt.imshow(I_new)\n",
    "plt.scatter(u1[inliers,0][::skip],u1[inliers,1][::skip])\n",
    "plt.scatter(u2[inliers,0][::skip]+w,u2[inliers,1][::skip])\n",
    "[plt.plot([u1[0],u2[0]+w],[u1[1],u2[1]]) for u1,u2 in zip(u1[inliers][::skip],u2[inliers][::skip])]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the weird, bad matches have been eliminated.  \n",
    "\n",
    "Now that we have the essential matrix, we can recover the relative pose of the two cameras.  OpenCV has an easy function to do this as well, that corresponds to Szeliski Eq. 7.18 and Eq. 7.25.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in,R,t,_ = cv2.recoverPose(E,x1[inliers,:2],x2[inliers,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the pose recovery process solves an equation that has four roots: to select the correct one, it uses the original points to enfore *chirality*, or the notion that the points in the second image should be in front of the camera.  Note also, that recoverPose only returns a single rotation and translation.  These correspond to the rotation and translation values for the second image: it is assumed that the first image has rotation given by the identity, and that the camera center is at $\\mathbf{X} = \\mathbf{0}$.\n",
    "\n",
    "We can now form the camera matrices $P_1 = [\\mathbf{I}| \\mathbf{0}]$ and $P_2 = [\\mathbf{R}|\\mathbf{t}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_1 = np.array([[1,0,0,0],\n",
    "                [0,1,0,0],\n",
    "                [0,0,0,1]])\n",
    "P_2 = np.hstack((R,t))\n",
    "print(P_1,P_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are projection matrices in generalized image coordinates.  We can always get back to camera coordinates by multiplying them by the camera matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_1c = K_cam @ P_1\n",
    "P_2c = K_cam @ P_2\n",
    "print(P_1c)\n",
    "print(P_2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task is to apply this code to two images of your (judicious) choosing.  After finding the two camera matrices, instantiate two camera models with these matrices, and then use your triangulation code to find the 3D position of the points of correspondence (only the inliers!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E,inliers = cv2.findEssentialMat(uv1,uv2,K,method=cv2.RANSAC,threshold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the essential matrix is found, we can recover the translation and rotation matrix up to a scale using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in,R,t,_ = cv2.recoverPose(E,uv1,uv2,mask=inliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that this function only returns a single rotation and translation: this method assumes that the first camera has canonical pose $\\mathbf{t} = \\mathbf{0}$ and $\\mathbf{R} = \\mathbf{I}$.  Alteratively, we can immediately define the camera matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_0 = K @ np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0]])\n",
    "P_1 = K @ np.hstack((R,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $P_0$ and $P_1$ are the camera matrices, and $K$ is the matrix of camera intrinsics\n",
    "$$\n",
    "K = \\begin{bmatrix} f & 0 & c_u \\\\\n",
    "                    0 & f & c_v \\\\\n",
    "                    0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
